{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"><font face=\"Bookman Old Style\"><b><center>Question 1</center></b><br>\n",
    "<font size=\"4\">1. For the text \"I am learning NLP\", build One Hot Encoded feature vectors using:<br>\n",
    "a) Pandas<br>\n",
    "b) Scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   I  am  learning  NLP\n",
      "0  1   0         0    0\n",
      "1  0   1         0    0\n",
      "2  0   0         1    0\n",
      "3  0   0         0    1\n"
     ]
    }
   ],
   "source": [
    "#1a\n",
    "import pandas as pd\n",
    "\n",
    "text = \"I am learning NLP\"\n",
    "words = text.split()\n",
    "df = pd.DataFrame()\n",
    "for word in words:\n",
    "    df[word] = [1 if w == word else 0 for w in words]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x0_I  x0_NLP  x0_am  x0_learning\n",
      "0   1.0     0.0    0.0          0.0\n",
      "1   0.0     0.0    1.0          0.0\n",
      "2   0.0     0.0    0.0          1.0\n",
      "3   0.0     1.0    0.0          0.0\n"
     ]
    }
   ],
   "source": [
    "#1b\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "onehot = OneHotEncoder()\n",
    "text = \"I am learning NLP\"\n",
    "words = text.split()\n",
    "words = [[word] for word in words]\n",
    "onehot_matrix = onehot.fit_transform(words).toarray()\n",
    "df = pd.DataFrame(onehot_matrix, columns=onehot.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"><font face=\"Bookman Old Style\"><b><center>Question 2</center></b><br>\n",
    "<font size=\"4\">2. Take a corpus with 4 sentences. Create Bag of Words feature vectors using Scikit:<br>\n",
    "a) Print feature names of the corpus.<br>\n",
    "b) Print the feature vectors matrix using DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names:\n",
      "['agra', 'and', 'billion', 'country', 'cuisine', 'cultural', 'delicious', 'diverse', 'famous', 'flavors', 'for', 'heritage', 'in', 'india', 'indian', 'is', 'its', 'known', 'landmarks', 'located', 'mahal', 'most', 'of', 'one', 'over', 'people', 'populous', 'renowned', 'rich', 'second', 'spices', 'taj', 'the', 'traditions', 'use', 'with', 'world']\n"
     ]
    }
   ],
   "source": [
    "#2a\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "corpus = [\n",
    "    \"India is known for its rich cultural heritage and diverse traditions.\",\n",
    "    \"The Taj Mahal, located in Agra, is one of India's most famous landmarks.\",\n",
    "    \"Indian cuisine is renowned for its delicious flavors and use of spices.\",\n",
    "    \"India is the world's second-most populous country, with over 1.3 billion people.\",\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "print(\"Feature Names:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Vectors Matrix:\n",
      "   agra  and  billion  country  cuisine  cultural  delicious  diverse  famous  \\\n",
      "0     0    1        0        0        0         1          0        1       0   \n",
      "1     1    0        0        0        0         0          0        0       1   \n",
      "2     0    1        0        0        1         0          1        0       0   \n",
      "3     0    0        1        1        0         0          0        0       0   \n",
      "\n",
      "   flavors  ...  renowned  rich  second  spices  taj  the  traditions  use  \\\n",
      "0        0  ...         0     1       0       0    0    0           1    0   \n",
      "1        0  ...         0     0       0       0    1    1           0    0   \n",
      "2        1  ...         1     0       0       1    0    0           0    1   \n",
      "3        0  ...         0     0       1       0    0    1           0    0   \n",
      "\n",
      "   with  world  \n",
      "0     0      0  \n",
      "1     0      0  \n",
      "2     0      0  \n",
      "3     1      1  \n",
      "\n",
      "[4 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "#2b\n",
    "X_array = X.toarray()\n",
    "df = pd.DataFrame(X_array, columns=feature_names)\n",
    "print(\"Feature Vectors Matrix:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"><font face=\"Bookman Old Style\"><b><center>Question 3</center></b><br>\n",
    "<font size=\"4\">3. For the text “I learn NLP because it is awesome”, print uni-grams, bi-grams and tri-grams using:<br>\n",
    "a) Textblob<br>\n",
    "b) Ntlk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-grams:\n",
      "['I', 'learn', 'NLP', 'because', 'it', 'is', 'awesome']\n",
      "\n",
      "Bi-grams:\n",
      "[WordList(['I', 'learn']), WordList(['learn', 'NLP']), WordList(['NLP', 'because']), WordList(['because', 'it']), WordList(['it', 'is']), WordList(['is', 'awesome'])]\n",
      "\n",
      "Tri-grams:\n",
      "[WordList(['I', 'learn', 'NLP']), WordList(['learn', 'NLP', 'because']), WordList(['NLP', 'because', 'it']), WordList(['because', 'it', 'is']), WordList(['it', 'is', 'awesome'])]\n"
     ]
    }
   ],
   "source": [
    "#3a\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I learn NLP because it is awesome\"\n",
    "blob = TextBlob(text)\n",
    "\n",
    "print(\"Uni-grams:\")\n",
    "print(blob.words)\n",
    "\n",
    "print(\"\\nBi-grams:\")\n",
    "print(blob.ngrams(n=2))\n",
    "\n",
    "print(\"\\nTri-grams:\")\n",
    "print(blob.ngrams(n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-grams:\n",
      "['I', 'learn', 'NLP', 'because', 'it', 'is', 'awesome']\n",
      "\n",
      "Bi-grams:\n",
      "[('I', 'learn'), ('learn', 'NLP'), ('NLP', 'because'), ('because', 'it'), ('it', 'is'), ('is', 'awesome')]\n",
      "\n",
      "Tri-grams:\n",
      "[('I', 'learn', 'NLP'), ('learn', 'NLP', 'because'), ('NLP', 'because', 'it'), ('because', 'it', 'is'), ('it', 'is', 'awesome')]\n"
     ]
    }
   ],
   "source": [
    "#3b\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"I learn NLP because it is awesome\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Uni-grams:\")\n",
    "print(tokens)\n",
    "\n",
    "print(\"\\nBi-grams:\")\n",
    "bi_grams = list(ngrams(tokens, 2))\n",
    "print(bi_grams)\n",
    "\n",
    "print(\"\\nTri-grams:\")\n",
    "tri_grams = list(ngrams(tokens, 3))\n",
    "print(tri_grams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"><font face=\"Bookman Old Style\"><b><center>Question 4</center></b><br>\n",
    "<font size=\"4\">4. For the corpus ['I like NLP', 'I am exploring NLP', 'I am a beginner in NLP', 'I want to learn NLP', 'I\n",
    "like advanced NLP'], create TFIDF feature vectors using Scikit:<br>\n",
    "a) Print vocabulary of the corpus.<br>\n",
    "b) Print the inverse document frequencies.<br>\n",
    "c) Print the feature vectors matrix using DataFrame.<br>\n",
    "d) Print cosine similarity of the 2nd sentence with all other sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "corpus = [\n",
    "    'I like NLP',\n",
    "    'I am exploring NLP',\n",
    "    'I am a beginner in NLP',\n",
    "    'I want to learn NLP',\n",
    "    'I like advanced NLP'\n",
    "]\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "['advanced', 'am', 'beginner', 'exploring', 'in', 'learn', 'like', 'nlp', 'to', 'want']\n"
     ]
    }
   ],
   "source": [
    "#4a\n",
    "vocabulary = tfidf_vectorizer.get_feature_names()\n",
    "print(\"Vocabulary:\")\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse Document Frequencies (IDF):\n",
      "[2.09861229 1.69314718 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 1.69314718 1.         2.09861229 2.09861229]\n"
     ]
    }
   ],
   "source": [
    "#4b\n",
    "print(\"Inverse Document Frequencies (IDF):\")\n",
    "print(tfidf_vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Vectors Matrix:\n",
      "   advanced        am  beginner  exploring        in     learn      like  \\\n",
      "0  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  0.861037   \n",
      "1  0.000000  0.588732  0.000000   0.729718  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.475575  0.589463   0.000000  0.589463  0.000000  0.000000   \n",
      "3  0.000000  0.000000  0.000000   0.000000  0.000000  0.556669  0.000000   \n",
      "4  0.729718  0.000000  0.000000   0.000000  0.000000  0.000000  0.588732   \n",
      "\n",
      "        nlp        to      want  \n",
      "0  0.508542  0.000000  0.000000  \n",
      "1  0.347715  0.000000  0.000000  \n",
      "2  0.280882  0.000000  0.000000  \n",
      "3  0.265256  0.556669  0.556669  \n",
      "4  0.347715  0.000000  0.000000  \n"
     ]
    }
   ],
   "source": [
    "#4c\n",
    "vocabulary = tfidf_vectorizer.get_feature_names()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vocabulary)\n",
    "print(\"Feature Vectors Matrix:\")\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity with 2nd Sentence:\n",
      "[[0.17682765 1.         0.37765328 0.09223325 0.12090552]]\n"
     ]
    }
   ],
   "source": [
    "#4d\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix[1:2], tfidf_matrix)\n",
    "print(\"Cosine Similarity with 2nd Sentence:\")\n",
    "print(cosine_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"><font face=\"Bookman Old Style\"><b><center>Question 5</center></b><br>\n",
    "<font size=\"4\">5. For the corpus [\"The quick brown fox jumps over the lazy dog\",\"The quick brown fox jumps over\n",
    "the quick dog\",\"The quick brown fox jumps over the quick brown dog\",\"The quick brown fox jumps\n",
    "over the quick brown fox\"] build the co-occurrence matrix and print it using DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       brown  dog  fox  jumps  lazy  over  quick  the\n",
      "brown     10    4    8      6     1     6     11   12\n",
      "dog        4    3    3      3     1     3      5    6\n",
      "fox        8    3    7      5     1     5      9   10\n",
      "jumps      6    3    5      4     1     4      7    8\n",
      "lazy       1    1    1      1     1     1      1    2\n",
      "over       6    3    5      4     1     4      7    8\n",
      "quick     11    5    9      7     1     7     13   14\n",
      "the       12    6   10      8     2     8     14   16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"The quick brown fox jumps over the quick dog\",\n",
    "    \"The quick brown fox jumps over the quick brown dog\",\n",
    "    \"The quick brown fox jumps over the quick brown fox\"\n",
    "]\n",
    "bow = CountVectorizer()\n",
    "bow_matrix = bow.fit_transform(corpus)\n",
    "co_occurrence_matrix = bow_matrix.T.dot(bow_matrix)\n",
    "vocab = bow.get_feature_names()\n",
    "df = pd.DataFrame(co_occurrence_matrix.toarray(), columns=vocab, index=vocab)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
